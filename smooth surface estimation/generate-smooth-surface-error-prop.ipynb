{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef34b1d6",
   "metadata": {},
   "source": [
    "This notebook pulls 2010 census data and attempts to generate a large set of points that approximates a smooth surface while propogating error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be91a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare static variables\n",
    "\n",
    "n=5 # The number of points to assign to each census block\n",
    "BINOMIAL_TRIALS = 40 # The number of trials in the binomial distribution used for weighting points in blocks. The higher the value, the more evenly distributed the population points will be through the census block\n",
    "BINOMIAL_SUCCESS = 0.5 # The probability of success for each trial in he weight assignment. Must be <=1. Use 1 for a uniform distribution\n",
    "\n",
    "# Potential other methods for weight generation:\n",
    "# Apply a transformation to make the outcome more normal\n",
    "# Apply a normal distribution and use min/max normalization\n",
    "# Weighted Poisson binomial distribution\n",
    "\n",
    "\n",
    "# Binomial distribution is nearly normal if np(1-p) >= 10\n",
    "print(BINOMIAL_TRIALS * BINOMIAL_SUCCESS * (1 - BINOMIAL_SUCCESS) >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import folium\n",
    "from IPython.display import clear_output\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d216b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request shapefile data for 2010 census tracts and convert to geopandas dataframe\n",
    "\n",
    "# Shapefile url\n",
    "data_url = 'https://www2.census.gov/geo/tiger/GENZ2010/gz_2010_10_140_00_500k.zip'\n",
    "\n",
    "\n",
    "# Request data\n",
    "data = requests.get(data_url)\n",
    "# convert to pandas dataframe\n",
    "tract_data = geopandas.read_file(BytesIO(data.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1f321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request shapefile data for 2010 census blocks and convert to geopandas dataframe\n",
    "\n",
    "# Shapefile url\n",
    "data_url = 'https://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_10_tabblock10.zip'\n",
    "\n",
    "\n",
    "# Request data\n",
    "data = requests.get(data_url)\n",
    "# convert to pandas dataframe\n",
    "block_data = geopandas.read_file(BytesIO(data.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab6825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each census block, create a bounding box\n",
    "block_bounds = block_data[\"geometry\"].bounds\n",
    "\n",
    "# Attch GEOID to boundaries\n",
    "block_bounds = block_data[[\"GEOID10\",\"geometry\"]].merge(block_bounds, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacd729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a 2D Binomial distribution over the bounding boxes\n",
    "\n",
    "# Takes in a row of 'block_bounds' and outputs a 2D Gaussian distribution of 'n' points over the bounding box, as well as the GEOID\n",
    "def get_points(row,n):\n",
    "    print(f\"Processing Block {row['GEOID10']}...\")\n",
    "    # 'i' is the total number of points left to assign\n",
    "    i=n\n",
    "    # 'points_return' is the list of all points for the block\n",
    "    # TODO: CRS is hardcoded\n",
    "    points_return = geopandas.GeoSeries(crs=\"EPSG:4269\")\n",
    "    # Allocate points until n have been assigned\n",
    "    while i > 0:\n",
    "        # Generates a uniform distribution for the y-axis located at the center of the box\n",
    "        pointsy = np.random.uniform(low=row[\"miny\"], high=row[\"maxy\"], size=i)\n",
    "        # Generates a uniform distribution for the x-axis located at the center of the box\n",
    "        pointsx = np.random.uniform(low=row[\"minx\"], high=row[\"maxx\"], size=i)\n",
    "        # Convert the points to Shapely points\n",
    "        points = geopandas.GeoSeries(geopandas.points_from_xy(pointsx, pointsy, crs=\"EPSG:4269\"))\n",
    "        # Check if the points are inside the block\n",
    "        point_checks = points.within(row[\"geometry\"])\n",
    "        # Add found points to our list\n",
    "        points_return = geopandas.GeoSeries(pd.concat([points_return, points[point_checks]], ignore_index=True), crs=points_return.crs)\n",
    "        # Set 'i' equal to the number of missed points\n",
    "        i = n - points_return.size\n",
    "    \n",
    "    # Generates a binomial distribution of weights\n",
    "    weights = np.random.binomial(n=BINOMIAL_TRIALS, p=BINOMIAL_SUCCESS, size=n)\n",
    "    # Normalize weights so that they sum to 1\n",
    "    weights = weights / np.sum(weights)\n",
    "    # Sort the weights based on distance from the mean\n",
    "    weights = weights[np.argsort(np.abs(weights - np.mean(weights)))]\n",
    "    # Generate a series containing the distance from each point to the centroid\n",
    "    distances = points_return.distance(row[\"geometry\"].centroid)\n",
    "    # Create a column for the index of the point and sort by distance\n",
    "    distances = distances.reset_index(name=\"distance\").sort_values(by=\"distance\")\n",
    "    # Assign a weight to each point\n",
    "    distances[\"weight\"] = weights\n",
    "    # Merge weights onto points\n",
    "    points_return = pd.merge(left=points_return.rename(\"geometry\"), right=distances, how=\"left\", right_on=\"index\", left_index=True)[[\"geometry\", \"weight\"]]\n",
    "    # Clear warnings from notebook output to prevent crash\n",
    "    clear_output()\n",
    "    # Return an array with every point in the cloud, the weights for each point and the GEOID\n",
    "    return list(chain(points_return[\"geometry\"].values, points_return[\"weight\"].values, [row[\"GEOID10\"]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian distribution to each block\n",
    "point_cloud = block_bounds.apply(get_points, axis=1, args=(n,), result_type='expand')\n",
    "\n",
    "# Rename columns of the pointcloud\n",
    "point_cloud.columns = ['point_' + str(x) if x<n else 'weight_' + str(x-n) if x<2*n else 'GEOID' for x in point_cloud.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull population data for 2010 Census blocks\n",
    "# Define request parameters\n",
    "\n",
    "year = '2010' # Year of interest\n",
    "datasource = 'dec' # Survey name\n",
    "subsource = 'pl' # Subsurvey name\n",
    "GET = 'P001001' # Variables to query\n",
    "FOR = 'block:*' # for predicate\n",
    "IN = 'state:10&in=county:*&in=tract:*'\n",
    "\n",
    "# Filepath to your Census API key\n",
    "keyfile = 'CensusAPIKey.txt'\n",
    "\n",
    "# Formatted API call\n",
    "data_url = f'https://api.census.gov/data/{year}/{datasource}/{subsource}?get={GET}&for={FOR}&in={IN}'\n",
    "\n",
    "# Read Census key into 'api_key'\n",
    "with open(keyfile) as key:\n",
    "    api_key = key.read().strip()\n",
    "\n",
    "# Add key to url\n",
    "data_url = f'{data_url}&key={api_key}'\n",
    "\n",
    "# Request data and convert from json\n",
    "data = requests.get(data_url).json()\n",
    "# First entry in list is a list of variable names\n",
    "data = pd.DataFrame(data[1:], columns = data[0])\n",
    "\n",
    "# Rename columns to match shapefile pull\n",
    "data.rename(columns = {\"state\":\"STATEFP10\", \"county\":\"COUNTYFP10\", \"tract\":\"TRACTCE10\", \"block\":\"BLOCKCE10\"}, inplace=True)\n",
    "\n",
    "# Attach to block shapes\n",
    "block_data = block_data.merge(data, on=[\"STATEFP10\",\"COUNTYFP10\",\"TRACTCE10\",\"BLOCKCE10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152eebf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ad46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull population data for 2010 Census tracts\n",
    "# This notebook pulls population, white population, and female population\n",
    "# Define request parameters\n",
    "\n",
    "year = '2010' # Year of interest\n",
    "datasource = 'acs' # Survey name\n",
    "subsource = 'acs5' # Subsurvey name\n",
    "GET = 'B01003_001E,B01003_001EA,B01003_001M,B01003_001MA,B02001_002E,B02001_002EA,B02001_002M,B02001_002MA,B01001_026E,B01001_026EA,B01001_026M,B01001_026MA' # Variables to query\n",
    "FOR = 'tract:*' # for predicate\n",
    "IN = 'state:10' # in predicate\n",
    "\n",
    "\n",
    "# Filepath to your Census API key\n",
    "keyfile = 'CensusAPIKey.txt'\n",
    "\n",
    "# Formatted API call\n",
    "data_url = f'https://api.census.gov/data/{year}/{datasource}/{subsource}?get={GET}&for={FOR}&in={IN}'\n",
    "\n",
    "# Read Census key into 'api_key'\n",
    "with open(keyfile) as key:\n",
    "    api_key = key.read().strip()\n",
    "\n",
    "# Add key to url\n",
    "data_url = f'{data_url}&key={api_key}'\n",
    "\n",
    "# Request data and convert from json\n",
    "data = requests.get(data_url).json()\n",
    "# First entry in list is a list of variable names\n",
    "data = pd.DataFrame(data[1:], columns = data[0])\n",
    "\n",
    "# Rename columns to match shapefile pull\n",
    "data.rename(columns = {\"state\":\"STATE\", \"county\":\"COUNTY\", \"tract\":\"TRACT\"}, inplace=True)\n",
    "\n",
    "# Attach to tract shapes\n",
    "tract_data = tract_data.merge(data, on=[\"STATE\",\"COUNTY\",\"TRACT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075c32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cc2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data anotations for errors\n",
    "print(tract_data.loc[~tract_data[\"B01003_001EA\"].isnull()])\n",
    "print(tract_data.loc[~tract_data[\"B01003_001MA\"].isnull()])\n",
    "print(tract_data.loc[~tract_data[\"B02001_002EA\"].isnull()])\n",
    "print(tract_data.loc[~tract_data[\"B02001_002MA\"].isnull()])\n",
    "print(tract_data.loc[~tract_data[\"B01001_026EA\"].isnull()])\n",
    "print(tract_data.loc[~tract_data[\"B01001_026MA\"].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for missing tract data\n",
    "\n",
    "tract_data.loc[pd.isna(tract_data[['B01003_001E','B01003_001M','B02001_002E','B02001_002M','B01001_026E','B01001_026M']]).any(axis=1)][['B01003_001E','B01003_001M','B02001_002E','B02001_002M','B01001_026E','B01001_026M']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a fraction of the population of each block as a value to each point\n",
    "\n",
    "# Merge each point to the 2010 census block containing it\n",
    "population_per_point = point_cloud.merge(block_data, how=\"left\", left_on=\"GEOID\", right_on=\"GEOID10\")\n",
    "\n",
    "# Multiply each weight by the block population to get the block population per point\n",
    "population_per_point[[x for x in population_per_point.columns if 'weight' in x]] = population_per_point[[x for x in population_per_point.columns if 'weight' in x]].mul(population_per_point[\"P001001\"].astype(int), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcede88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten to a GeoSeries where each row is a point and its weight\n",
    "weights = np.array([[row[\"weight_\" + str(i)] for i in range(n)] for _, row in population_per_point.iterrows()]).flatten()\n",
    "points = np.array([[row[\"point_\" + str(i)] for i in range(n)] for _, row in population_per_point.iterrows()]).flatten()\n",
    "points_list = geopandas.GeoDataFrame({\"population_per_point\":weights,\"geometry\":points}, crs=\"EPSG:4269\")\n",
    "\n",
    "\n",
    "# Determine the number of points in the point cloud. This should be n * the number of census blocks\n",
    "print(points_list.shape[0] / n == block_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatially join each point to the 2010 census tract containing it\n",
    "variables_per_point = geopandas.sjoin(points_list, tract_data, how=\"left\", op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for missing data: ~750 points with missed in the join\n",
    "variables_per_point.loc[pd.isna(variables_per_point[['B01003_001E','B01003_001M','B02001_002E','B02001_002M','B01001_026E','B01001_026M',\"index_right\"]]).any(axis=1)][['B01003_001E','B01003_001M','B02001_002E','B02001_002M','B01001_026E','B01001_026M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Plot is large and should only be rendered if necessary\n",
    "# TODO: Points around the edge of the state are being lost\n",
    "\n",
    "\"\"\"\n",
    "# Find and plot all missed points \n",
    "missed_points = variables_per_point.loc[variables_per_point[\"index_right\"].isna()]\n",
    "\n",
    "# initialize the map and store it in a folium map object\n",
    "us_map = folium.Map(location=[38.9108, -75.5277], zoom_start=8, tiles=None)\n",
    "\n",
    "# Add background tiles\n",
    "folium.TileLayer('CartoDB positron',name=\"Light Map\",control=False).add_to(us_map)\n",
    "\n",
    "# Style and highlight functions map population values to color values\n",
    "style_function = lambda x: {\"weight\":0.5, \n",
    "                            'color':'black',\n",
    "                            'fillColor':'red', \n",
    "                            'fillOpacity':0.75}\n",
    "\n",
    "# Add a map over the tiles with the given colors and a tooltip\n",
    "NIL=folium.features.GeoJson(\n",
    "        missed_points, # Full geopandas data\n",
    "        style_function=style_function, # function for base colors\n",
    "        control=False\n",
    "    )\n",
    "\n",
    "# Add elements to map\n",
    "us_map.add_child(NIL)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~175 points with 0 population\n",
    "variables_per_point.loc[variables_per_point[\"B01003_001E\"].astype(float) <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f2e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude missed points from the list\n",
    "variables_per_point = variables_per_point.dropna(subset = [\"B01003_001E\", \"B02001_002E\", \"B01001_026E\",\"B01003_001M\", \"B02001_002M\", \"B01001_026M\"])\n",
    "# Exclude points corresponding to a population of 0\n",
    "variables_per_point = variables_per_point.loc[variables_per_point[\"B01003_001E\"].astype(float) > 0]\n",
    "\n",
    "\n",
    "# Divide variables of intersest by tract population and multiply by the portion of the population represented by each point\n",
    "variables_per_point[[\"B01003_001E\", \"B02001_002E\", \"B01001_026E\"]] = variables_per_point[[\"B01003_001E\", \"B02001_002E\", \"B01001_026E\"]].astype(int).div(variables_per_point[\"B01003_001E\"].astype(int), axis=0).mul(variables_per_point[\"population_per_point\"].astype(float), axis=0)\n",
    "\n",
    "# Propogate Error through ratios: dg = g(a,b) sqrt([da/a]^2 + [db/b]^2)\n",
    "variables_per_point[\"B02001_002T\"] = variables_per_point[\"B02001_002E\"].astype(int).div(variables_per_point[\"B01003_001E\"].astype(int), axis=0).mul((variables_per_point[\"B02001_002M\"].astype(float).div(variables_per_point[\"B02001_002E\"].astype(float)).apply(np.square) + variables_per_point[\"B01003_001M\"].astype(float).div(variables_per_point[\"B01003_001E\"].astype(float)).apply(np.square)).apply(np.sqrt), axis=0)\n",
    "# Propogate error for multiplication with weights: df_n = w_n * x_n * dg\n",
    "variables_per_point[\"B02001_002T\"] = variables_per_point[\"B02001_002T\"].mul(variables_per_point[\"population_per_point\"], axis=0)\n",
    "\n",
    "# Repeat for other variables\n",
    "# Propogate Error through ratios: dg = g(a,b) sqrt([da/a]^2 + [db/b]^2)\n",
    "variables_per_point[\"B01001_026T\"] = variables_per_point[\"B01001_026E\"].astype(int).div(variables_per_point[\"B01003_001E\"].astype(int), axis=0).mul((variables_per_point[\"B01001_026M\"].astype(float).div(variables_per_point[\"B01001_026E\"].astype(float)).apply(np.square) + variables_per_point[\"B01003_001M\"].astype(float).div(variables_per_point[\"B01003_001E\"].astype(float)).apply(np.square)).apply(np.sqrt), axis=0)\n",
    "# Propogate error for multiplication with weights: df_n = w_n * x_n * dg\n",
    "variables_per_point[\"B01001_026T\"] = variables_per_point[\"B01001_026T\"].mul(variables_per_point[\"population_per_point\"], axis=0)\n",
    "\n",
    "# For population, we do not have a way to propogate error since we are summing block data\n",
    "\n",
    "# Reset index\n",
    "variables_per_point = variables_per_point.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611876df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b0f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of points missed in the transfer of data from tracts to points\n",
    "print(points_list.shape[0] -  variables_per_point.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36075b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request shapefile data for 2020 census tracts and convert to geopandas dataframe\n",
    "\n",
    "# Shapefile url\n",
    "data_url = 'https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_10_tract_500k.zip'\n",
    "\n",
    "\n",
    "# Request data\n",
    "data = requests.get(data_url)\n",
    "# convert to pandas dataframe\n",
    "tract2020 = geopandas.read_file(BytesIO(data.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f754e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc3b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959088ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatially join points to 2020 census tracts\n",
    "interpolated_values = geopandas.sjoin(variables_per_point[[\"geometry\",\"B01003_001E\", \"B02001_002E\", \"B01001_026E\",\"B01003_001M\", \"B02001_002T\", \"B01001_026T\"]], tract2020, how=\"right\", op='within')\n",
    "\n",
    "# Assign 0 value to tracts with no points\n",
    "interpolated_values = interpolated_values.fillna(value=0)\n",
    "\n",
    "# Propogate error through the summation of points: dh_m = sqrt( sum_{i=1}^n delta_i dg^2) : where delta_i=1 if the it point is in the mth 2020 census tract and delta_i=0 otherwise\n",
    "interpolated_errors = interpolated_values[[\"B01003_001M\", \"B02001_002T\", \"B01001_026T\"]].astype(float).apply(np.square)\n",
    "interpolated_errors[\"GEOID\"] = interpolated_values[\"GEOID\"]\n",
    "interpolated_errors = (interpolated_errors.groupby(\"GEOID\").sum()).apply(np.sqrt)\n",
    "\n",
    "# Sum the values for each 2020 tract\n",
    "interpolated_values = interpolated_values[[\"GEOID\", \"B01003_001E\", \"B02001_002E\", \"B01001_026E\"]].groupby(\"GEOID\").sum()\n",
    "\n",
    "# Join values and errors into a completed dataframe\n",
    "final_df = interpolated_values.join(interpolated_errors, on=\"GEOID\")\n",
    "\n",
    "# Reset GEOID from index to column\n",
    "final_df = final_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14da95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull population data for 2020 Census tracts\n",
    "# Define request parameters\n",
    "\n",
    "year = '2020' # Year of interest\n",
    "datasource = 'acs' # Survey name\n",
    "subsource = 'acs5' # Subsurvey name\n",
    "GET = 'B01003_001E,B01003_001M,B02001_002E,B02001_002M,B01001_026E,B01001_026M' # Variables to query\n",
    "FOR = 'tract:*' # for predicate\n",
    "IN = 'state:10' # in predicate\n",
    "\n",
    "\n",
    "# Filepath to your Census API key\n",
    "keyfile = 'CensusAPIKey.txt'\n",
    "\n",
    "# Formatted API call\n",
    "data_url = f'https://api.census.gov/data/{year}/{datasource}/{subsource}?get={GET}&for={FOR}&in={IN}'\n",
    "\n",
    "# Read Census key into 'api_key'\n",
    "with open(keyfile) as key:\n",
    "    api_key = key.read().strip()\n",
    "\n",
    "# Add key to url\n",
    "data_url = f'{data_url}&key={api_key}'\n",
    "\n",
    "# Request data and convert from json\n",
    "data = requests.get(data_url).json()\n",
    "# First entry in list is a list of variable names\n",
    "tract2020_data = pd.DataFrame(data[1:], columns = data[0])\n",
    "\n",
    "# Add a GEOID column to the data\n",
    "tract2020_data[\"GEOID\"] = tract2020_data[\"state\"].astype(str) + tract2020_data[\"county\"].astype(str) +tract2020_data[\"tract\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701bef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.merge(tract2020_data, left_on=\"GEOID\", right_on=\"GEOID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write combined dataframe of 2020 ground truth and estimated values to a csv\n",
    "final_df.merge(tract2020_data, left_on=\"GEOID\", right_on=\"GEOID\").to_csv(\"estimates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbe802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0c4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47c033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d11532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db34f8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28701c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9f669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461423b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
